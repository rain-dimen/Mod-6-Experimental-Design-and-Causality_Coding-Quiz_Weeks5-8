{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b2554e",
   "metadata": {},
   "source": [
    "# Mod 6 Homework Reflections \n",
    "# Weeks 5-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa913016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (1.16.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [statsmodels]\u001b[0m [statsmodels]\n",
      "\u001b[1A\u001b[2KSuccessfully installed patsy-1.0.1 statsmodels-0.14.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5ccdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb5b35d-d0f3-4154-b67e-185ba285484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baed688",
   "metadata": {},
   "source": [
    "# Week 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323dddb",
   "metadata": {},
   "source": [
    "1. Draw a diagram for the following negative feedback loop:\n",
    "\n",
    "Sweating causes body temperature to decrease.  High body temperature causes sweating.\n",
    "\n",
    "A negative feedback loop means that one thing increases another while the second thing decreases the first.\n",
    "\n",
    "Remember that we are using directed acyclic graphs where two things cannot directly cause each other.\n",
    "\n",
    "2. Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.\n",
    "\n",
    "3. Draw a diagram for the following situation:\n",
    "\n",
    "Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.\n",
    "Bears eat deer, decreasing their population.\n",
    "Deer eat flowers, decreasing their population.\n",
    "\n",
    "Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.\n",
    "\n",
    "Identify a backdoor path with one or more confounders for the relationship between deer and flowers.\n",
    "\n",
    "4. Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df460110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lightning      bears       deer    flowers\n",
      "0   5.993428  56.683461  40.024597  57.853220\n",
      "1   4.723471  71.759312  39.478018  46.398948\n",
      "2   6.295377  42.644146  36.028023  53.076502\n",
      "3   8.046060  38.446368  25.001107  69.666813\n",
      "4   4.531693  60.493241  42.684678  45.933511\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "\n",
    "# Simulating the data \n",
    "\n",
    "np.random.seed(42)  \n",
    "n = 500  # observations\n",
    "\n",
    "# Exogenous variable: Lightning storms, frequency/intensity \n",
    "lightning = np.random.normal(5, 2, n)\n",
    "\n",
    "# Bears are frightened away by lightning\n",
    "bears = 100 - 8 * lightning + np.random.normal(0, 5, n)\n",
    "\n",
    "# Deer are frightened by lightning and eaten by bears\n",
    "deer = 80 - 5 * lightning - 0.3 * bears + np.random.normal(0, 5, n)\n",
    "\n",
    "# Flowers: grow more with lightning, eaten by deer\n",
    "flowers = 50 + 4 * lightning - 0.5 * deer + np.random.normal(0, 5, n)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"lightning\": lightning,\n",
    "    \"bears\": bears,\n",
    "    \"deer\": deer,\n",
    "    \"flowers\": flowers\n",
    "})\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34998ee4",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee53e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (ATE): 1.6979\n",
      "Average Treatment Effect on Treated (ATT): 1.8464\n",
      "Average Treatment Effect on Untreated (ATU): 1.5495\n",
      "Naive Optimal Treatment Effect (max): 2.1725\n",
      "Robust Marginal Treatment Effect (90th percentile): 1.9280\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "df6 = pd.read_csv(\"homework_6.1.csv\")\n",
    "\n",
    "# Create treated and untreated groups\n",
    "treated = df6[df6[\"X\"] == 1].reset_index(drop=True)\n",
    "untreated = df6[df6[\"X\"] == 0].reset_index(drop=True)\n",
    "\n",
    "# Define nearest-neighbor match function\n",
    "def nearest_match(source, target):\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(target[[\"Z\"]])\n",
    "    distances, indices = nn.kneighbors(source[[\"Z\"]])\n",
    "    return indices.flatten()\n",
    "\n",
    "# Match treated to untreated and untreated to treated\n",
    "treated_to_untreated_idx = nearest_match(treated, untreated)\n",
    "untreated_to_treated_idx = nearest_match(untreated, treated)\n",
    "\n",
    "# Generate counterfactual outcomes\n",
    "treated[\"Y_cf\"] = untreated.loc[treated_to_untreated_idx, \"Y\"].values\n",
    "untreated[\"Y_cf\"] = treated.loc[untreated_to_treated_idx, \"Y\"].values\n",
    "\n",
    "# Compute treatment effects\n",
    "treated[\"TE\"] = treated[\"Y\"] - treated[\"Y_cf\"]\n",
    "untreated[\"TE\"] = treated.loc[untreated_to_treated_idx, \"Y\"].values - untreated[\"Y\"]\n",
    "\n",
    "# Compute average treatment effects\n",
    "ate = (treated[\"TE\"].mean() + untreated[\"TE\"].mean()) / 2\n",
    "att = treated[\"TE\"].mean()\n",
    "atu = untreated[\"TE\"].mean()\n",
    "\n",
    "# Compute Optimal Treatment Effect (max) and MTE using the 90th percentile\n",
    "optimal_te = untreated[\"TE\"].max()\n",
    "MTE_90th = np.percentile(untreated[\"TE\"], 90)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average Treatment Effect (ATE): {ate:.4f}\")\n",
    "print(f\"Average Treatment Effect on Treated (ATT): {att:.4f}\")\n",
    "print(f\"Average Treatment Effect on Untreated (ATU): {atu:.4f}\")\n",
    "print(f\"Naive Optimal Treatment Effect (max): {optimal_te:.4f}\")\n",
    "print(f\"Robust Marginal Treatment Effect (90th percentile): {MTE_90th:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6478f6",
   "metadata": {},
   "source": [
    "# Week 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9bd3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True effect of X on Y: 2.0\n",
      "Without Confounder W: 3.510405682088037\n",
      "With Confounder W: 1.9981919455639527\n"
     ]
    }
   ],
   "source": [
    "# Week 7, Question 1\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 1000\n",
    "\n",
    "# Confounder\n",
    "W = np.random.normal(0, 1, n)\n",
    "\n",
    "# Treatment\n",
    "X = W + np.random.normal(0, 1, n)\n",
    "\n",
    "# Outcome\n",
    "Y = 2*X + 3*W + np.random.normal(0, 1, n)\n",
    "\n",
    "# Regress Y and X - no confounder)\n",
    "model_no_conf = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "\n",
    "# Regress Y and X - including confounder\n",
    "model_full = sm.OLS(Y, sm.add_constant(np.column_stack([X, W]))).fit()\n",
    "\n",
    "print(\"True effect of X on Y: 2.0\")\n",
    "print(\"Without Confounder W:\", model_no_conf.params[1])\n",
    "print(\"With Confounder W:\", model_full.params[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a1bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest p-value for W: 0.002831\n",
      "Number of times p < 0.05: 51 out of 1000\n"
     ]
    }
   ],
   "source": [
    "# Week 7, Question 2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "p_values = []  \n",
    "\n",
    "for i in range(1000):\n",
    "    n = 200  # number of data points\n",
    "\n",
    "    W = np.random.normal(0, 1, n)\n",
    "    X = np.random.normal(0, 1, n)\n",
    "    error = np.random.normal(0, 1, n)\n",
    "\n",
    "    # true model, coeff of W is 0\n",
    "    Y = 2 * X + error  \n",
    "\n",
    "    # regression: including X and W\n",
    "    XW = sm.add_constant(np.column_stack((X, W)))\n",
    "    model = sm.OLS(Y, XW).fit()\n",
    "\n",
    "    # p-value of W is the 3rd value (after constant and X)\n",
    "    p_w = model.pvalues[2]\n",
    "    p_values.append(p_w)\n",
    "\n",
    "# results\n",
    "min_p = np.min(p_values)\n",
    "count_sig = np.sum(np.array(p_values) < 0.05)\n",
    "\n",
    "print(\"Smallest p-value for W:\", round(min_p, 6))\n",
    "print(\"Number of times p < 0.05:\", count_sig, \"out of 1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8544d41",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "Suppose that a process can be modeled via linear regression: \n",
    "\n",
    "\n",
    "W = np.random.normal(0, 1, (1000,))\n",
    "X = W + np.random.normal(0, 1, (1000,)) \n",
    "Z = np.random.normal(0, 1, (1000,)) \n",
    "Y = X + Z + W + np.random.normal(0, 1, (1000,))\n",
    " \n",
    "\n",
    "\n",
    "Which is closest to the correlation of ﻿X﻿ with the error term in the equation for ﻿Y﻿? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef952c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(0, 1, (1000,))\n",
    "X = W + np.random.normal(0, 1, (1000,)) \n",
    "Z = np.random.normal(0, 1, (1000,)) \n",
    "Y = X + Z + W + np.random.normal(0, 1, (1000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e85248",
   "metadata": {},
   "source": [
    "the error term is \n",
    "error = Z + W + np.random.normal(0, 1, num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ef8965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between X and the error term: 0.40335501644287275\n"
     ]
    }
   ],
   "source": [
    "W = np.random.normal(0, 1, (1000,))\n",
    "X = W + np.random.normal(0, 1, (1000,))\n",
    "Z = np.random.normal(0, 1, (1000,))\n",
    "Y = X + Z + W + np.random.normal(0, 1, (1000,))\n",
    "\n",
    "# when Y is modeled as depending only on X, the error term = Z + W + random noise\n",
    "error = Z + W + np.random.normal(0, 1, (1000,))\n",
    "\n",
    "# compute correlation between X and the error term\n",
    "corr = np.corrcoef(X, error)[0, 1]\n",
    "print(\"Correlation between X and the error term:\", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa775290",
   "metadata": {},
   "source": [
    "Question 2\n",
    "\n",
    "\n",
    "If ﻿Y﻿ is written as depending on ﻿X﻿ and ﻿Z﻿ only, ﻿W﻿ is part of the error term. Which, then, is closest to the expected correlation of ﻿X﻿ with the error term in the equation for ﻿Y﻿? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb2ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between X and the error term: 0.48407444829165097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "W = np.random.normal(0, 1, (1000,))\n",
    "X = W + np.random.normal(0, 1, (1000,))\n",
    "Z = np.random.normal(0, 1, (1000,))\n",
    "Y = X + Z + W + np.random.normal(0, 1, (1000,))\n",
    "\n",
    "# since Y depends on X and Z only, the error term contains W + random noise\n",
    "error = W + np.random.normal(0, 1, (1000,))\n",
    "\n",
    "# compute correlation between X and the error term\n",
    "corr = np.corrcoef(X, error)[0, 1]\n",
    "print(\"Correlation between X and the error term:\", corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4987e31",
   "metadata": {},
   "source": [
    "Question 3\n",
    "\n",
    "In the data frame for homework_7.1.csv, control for W by regressing ﻿Y﻿ on ﻿X﻿ and ﻿Z﻿ at the following constant values of ﻿W﻿: -1, 0, and 1. (You cannot literally use a constant value of ﻿W﻿, of course, or you will have only one data point! How will you manage this?) The question is: Is the coefficient of ﻿X﻿  \n",
    "\n",
    "Option A\n",
    "increasing\n",
    "\n",
    "Option B\n",
    "decreasing\n",
    "\n",
    "Option C\n",
    "staying about the same (say, within 0.2 or so) as ﻿W﻿ increases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebf0e890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>W</th>\n",
       "      <th>Z</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.137055</td>\n",
       "      <td>1.221768</td>\n",
       "      <td>0.327829</td>\n",
       "      <td>1.944532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.112905</td>\n",
       "      <td>0.465835</td>\n",
       "      <td>0.599650</td>\n",
       "      <td>0.655514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.077755</td>\n",
       "      <td>1.795414</td>\n",
       "      <td>-0.063393</td>\n",
       "      <td>5.934411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.456373</td>\n",
       "      <td>-0.512159</td>\n",
       "      <td>1.177413</td>\n",
       "      <td>-0.188064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.012402</td>\n",
       "      <td>0.080002</td>\n",
       "      <td>-0.275697</td>\n",
       "      <td>-0.533775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         X         W         Z         Y\n",
       "0           0  1.137055  1.221768  0.327829  1.944532\n",
       "1           1 -0.112905  0.465835  0.599650  0.655514\n",
       "2           2  2.077755  1.795414 -0.063393  5.934411\n",
       "3           3  0.456373 -0.512159  1.177413 -0.188064\n",
       "4           4 -1.012402  0.080002 -0.275697 -0.533775"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = pd.read_csv(\"homework_7.1.csv\")\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf1222e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When W is around -1, coefficient of X = 0.990090408694134\n",
      "When W is around 0, coefficient of X = 1.4859822514480032\n",
      "When W is around 1, coefficient of X = 1.9936504417092331\n"
     ]
    }
   ],
   "source": [
    "# Subset the data where W is close to -1\n",
    "df_w_neg1 = df7[(df7['W'] > -1.5) & (df7['W'] < -0.5)]\n",
    "model_neg1 = sm.OLS(df_w_neg1['Y'], sm.add_constant(df_w_neg1[['X', 'Z']])).fit()\n",
    "print(\"When W is around -1, coefficient of X =\", model_neg1.params['X'])\n",
    "\n",
    "# W around 0\n",
    "df_w_0 = df7[(df7['W'] > -0.5) & (df7['W'] < 0.5)]\n",
    "model_0 = sm.OLS(df_w_0['Y'], sm.add_constant(df_w_0[['X', 'Z']])).fit()\n",
    "print(\"When W is around 0, coefficient of X =\", model_0.params['X'])\n",
    "\n",
    "# W around 1\n",
    "df_w_1 = df7[(df7['W'] > 0.5) & (df7['W'] < 1.5)]\n",
    "model_1 = sm.OLS(df_w_1['Y'], sm.add_constant(df_w_1[['X', 'Z']])).fit()\n",
    "print(\"When W is around 1, coefficient of X =\", model_1.params['X'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ea707",
   "metadata": {},
   "source": [
    "Question 4\n",
    "\n",
    "\n",
    "def make_error(corr_const, num): \n",
    "\n",
    " err = list() \n",
    "\n",
    "    prev = np.random.normal(0, 1) \n",
    "\n",
    " for n in range(num): \n",
    "\n",
    "    prev = corr_const * prev + (1 - corr_const) * np.random.normal(0, 1) \n",
    "\n",
    "    err.append(prev) \n",
    "\n",
    "return np.array(err) \n",
    "\n",
    "\n",
    "\n",
    "Create a linear regression model that uses this function as the error for both (a) the treatment, ﻿X﻿, and (b) the outcome, ﻿Y﻿. (You can use random normal error for any other covariates, if you have them.) \n",
    "\n",
    "\n",
    "\n",
    "As corr_const increases from 0.2 to 0.5 to 0.8, find (i) the standard deviation of the estimate of the ﻿X﻿ coefficient over many trials, and (ii) the mean of the standard error estimate of the ﻿X﻿ coefficient over many trials. \n",
    "\n",
    "\n",
    "\n",
    "When corr_const increases, then: \n",
    "\n",
    "\n",
    "\n",
    "Hint: don't forget to include an intercept in your regression\n",
    "\n",
    "Option A\n",
    "(i) and (ii) remain about the same\n",
    "\n",
    "Option B\n",
    "The ratio (i) / (ii) decreases\n",
    "\n",
    "Option C\n",
    "(i) and (ii) differ, but their ratio remains about the same\n",
    "\n",
    "Option D\n",
    "The ratio (i) / (ii) increases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf85ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   corr_const  (i) std of β_X  (ii) mean SE  ratio (i)/(ii)\n",
      "0         0.2        0.029915      0.031531        0.948757\n",
      "1         0.5        0.041477      0.031510        1.316324\n",
      "2         0.8        0.067064      0.031621        2.120874\n"
     ]
    }
   ],
   "source": [
    "# Function to make correlated errors\n",
    "def make_error(corr_const, num):\n",
    "    err = []\n",
    "    prev = np.random.normal(0, 1)\n",
    "    for i in range(num):\n",
    "        prev = corr_const * prev + (1 - corr_const) * np.random.normal(0, 1)\n",
    "        err.append(prev)\n",
    "    return np.array(err)\n",
    "\n",
    "corr_levels = [0.2, 0.5, 0.8]\n",
    "\n",
    "# To store the results\n",
    "results = []\n",
    "\n",
    "# Fix random seed so we can reproduce results\n",
    "np.random.seed(42)\n",
    "\n",
    "# For each correlation level\n",
    "for corr_const in corr_levels:\n",
    "    all_betas = []\n",
    "    all_ses = []\n",
    "    \n",
    "    # Run many simulations\n",
    "    for i in range(100):  # 100 trials\n",
    "        n = 1000\n",
    "        \n",
    "        # Create errors for X and Y\n",
    "        eX = make_error(corr_const, n)\n",
    "        eY = make_error(corr_const, n)\n",
    "        \n",
    "        # Generate data\n",
    "        Z = np.random.normal(0, 1, n)\n",
    "        X = 0.5 * Z + eX\n",
    "        Y = 1.0 * X + 0.3 * Z + eY\n",
    "        \n",
    "        # Run regression: Y ~ X + Z\n",
    "        X_matrix = sm.add_constant(np.column_stack([X, Z]))\n",
    "        model = sm.OLS(Y, X_matrix).fit()\n",
    "        \n",
    "        # Save the coefficient for X and its standard error\n",
    "        all_betas.append(model.params[1])  # X coefficient\n",
    "        all_ses.append(model.bse[1])       # X standard error\n",
    "    \n",
    "    # Compute (i) and (ii)\n",
    "    std_dev_of_betas = np.std(all_betas)\n",
    "    mean_se = np.mean(all_ses)\n",
    "    ratio = std_dev_of_betas / mean_se\n",
    "    \n",
    "    # Save the results\n",
    "    results.append([corr_const, std_dev_of_betas, mean_se, ratio])\n",
    "\n",
    "# Show results\n",
    "results_df = pd.DataFrame(results, columns=[\"corr_const\", \"(i) std of β_X\", \"(ii) mean SE\", \"ratio (i)/(ii)\"])\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
